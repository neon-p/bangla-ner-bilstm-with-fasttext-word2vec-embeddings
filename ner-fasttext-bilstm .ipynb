{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport re\nimport pandas as pd \nfrom pprint import pprint\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, save_model\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nimport os\nfrom gensim.models import FastText,KeyedVectors\nfrom keras.utils import to_categorical\nimport time\nfrom sklearn.metrics import f1_score,confusion_matrix,classification_report\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-09-21T09:27:07.716633Z","iopub.execute_input":"2021-09-21T09:27:07.717061Z","iopub.status.idle":"2021-09-21T09:27:14.235007Z","shell.execute_reply.started":"2021-09-21T09:27:07.716932Z","shell.execute_reply":"2021-09-21T09:27:14.233899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/banglaner/label_data.txt\",encoding=\"utf-8\",sep=\" \",names=['word','label'],skip_blank_lines=False)\ndf=pd.DataFrame(data)\ndf.replace(np.NaN,\"Break\",inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T09:27:19.612663Z","iopub.execute_input":"2021-09-21T09:27:19.612986Z","iopub.status.idle":"2021-09-21T09:27:19.784141Z","shell.execute_reply.started":"2021-09-21T09:27:19.612958Z","shell.execute_reply":"2021-09-21T09:27:19.783055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking empty list\ndef remove_empty_list(data):\n    a=list(filter(lambda x: [] != x, data))\n    return a\n\n#for spiliting sentences\ndef making_list(data):\n    f=[]\n    t=[]\n    for i in data:\n        if (i==\"Break\"):\n            f.append(t)\n            t=[]\n        else:\n            t.append(i)\n    return remove_empty_list(f)\n\ndef data_preprocessing():\n    label=making_list(df['label'])\n    sen=making_list(df['word'])\n    l=[]\n    for i in sen:\n        s=\" \".join(i)\n        l.append(preprocessing(s).split())\n    max_len=max([len(i) for i in sen])\n    return sen,label,max_len\n\ndef preprocessing(sentence):\n    whitespace = re.compile(u\"[\\ufeff\\u200d\\u200b\\u200c\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n    fullspace = re.compile(u\"[\\s\\u0020]+\", re.UNICODE)\n    bangla_fullstop = u\"\\u0964\"\n    punctSeq   = u\"['\\\"“”‘’]+|[.?!,…]+|[:;]+\"\n    punc = u\"[(),$%^&*+={}\\[\\]:\\\"|\\'\\~`<>/,¦!?½£¶¼©⅐⅑⅒⅓⅔⅕⅖⅗⅘⅙⅚⅛⅜⅝⅞⅟↉¤¿º;-]+\"\n    sentence= whitespace.sub(\"\",sentence)\n    sentence= fullspace.sub(\" \",sentence)\n    sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', \"\", sentence, flags=re.MULTILINE)\n    sentence = re.sub(punctSeq, \"\", sentence)\n    sentence = re.sub(bangla_fullstop, \"\",sentence)\n    sentence = re.sub(punc, \"\", sentence)\n    return sentence","metadata":{"execution":{"iopub.status.busy":"2021-09-21T09:27:25.710472Z","iopub.execute_input":"2021-09-21T09:27:25.710892Z","iopub.status.idle":"2021-09-21T09:27:25.724165Z","shell.execute_reply.started":"2021-09-21T09:27:25.710845Z","shell.execute_reply":"2021-09-21T09:27:25.722647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sen,label,max_len=data_preprocessing()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T09:27:31.205459Z","iopub.execute_input":"2021-09-21T09:27:31.205894Z","iopub.status.idle":"2021-09-21T09:27:32.546566Z","shell.execute_reply.started":"2021-09-21T09:27:31.205864Z","shell.execute_reply":"2021-09-21T09:27:32.545381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating unique words/labels\nwords=list(set(df[\"word\"].values))\ntags = list(set(df[\"label\"].values))\ntags.remove('Break')\n\n#creating dict with label/words knwon as corpus dict\nword_to_int = dict((c, i) for i, c in enumerate(words))\nn_words=len(word_to_int)\n\nlabel_to_int = dict((c, i) for i, c in enumerate(tags))\nn_tags=len(label_to_int)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T09:27:35.142617Z","iopub.execute_input":"2021-09-21T09:27:35.142976Z","iopub.status.idle":"2021-09-21T09:27:35.168164Z","shell.execute_reply.started":"2021-09-21T09:27:35.142941Z","shell.execute_reply":"2021-09-21T09:27:35.166543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encoded the sentences\ndataX=[]\nfor i in sen:\n    l=[]\n    for j in i:\n        l.append(word_to_int[j])\n    dataX.append(l)\n    \n#padding sequences\nX = pad_sequences(maxlen=max_len, sequences=dataX, padding=\"post\",value=n_words - 1)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T09:27:41.217306Z","iopub.execute_input":"2021-09-21T09:27:41.217666Z","iopub.status.idle":"2021-09-21T09:27:41.402824Z","shell.execute_reply.started":"2021-09-21T09:27:41.217637Z","shell.execute_reply":"2021-09-21T09:27:41.401609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encoded the labels\ndataY=[]\nfor i in label:\n    l=[]\n    for j in i:\n        l.append(label_to_int[j])\n    dataY.append(l)\n\n#padding and categorical (for multilabel classification\n#it's required) sequences\nY = pad_sequences(maxlen=max_len, sequences=dataY, padding=\"post\",value=label_to_int[\"0\"])\ny = [to_categorical(i, num_classes=n_tags) for i in Y]","metadata":{"execution":{"iopub.status.busy":"2021-09-21T09:27:45.358955Z","iopub.execute_input":"2021-09-21T09:27:45.359593Z","iopub.status.idle":"2021-09-21T09:27:45.846916Z","shell.execute_reply.started":"2021-09-21T09:27:45.359547Z","shell.execute_reply":"2021-09-21T09:27:45.845657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pre-trained word embedding start here\n#loading pre-trained word embedding\nst=time.time()\npre_em_model= KeyedVectors.load_word2vec_format('../input/fastextbn/cc.bn.300.vec')\net=time.time()\ndt=et-st\nprint('dt=',dt)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T09:27:50.251028Z","iopub.execute_input":"2021-09-21T09:27:50.251506Z","iopub.status.idle":"2021-09-21T09:42:14.977382Z","shell.execute_reply.started":"2021-09-21T09:27:50.251476Z","shell.execute_reply":"2021-09-21T09:42:14.976258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating dictonary with pre-trained model words and vector\nembeddings_index={}\nfor word, vector in zip(pre_em_model.key_to_index, pre_em_model.vectors):\n    coefs = np.asarray(vector, dtype='float32')\n    embeddings_index[word] = coefs\n\n#creating embedding_matrix for loaded the 300 dim vector(this come form pre-trained model)\n#representation of corpus dict\nembedding_matrix = np.zeros((len(words),300),dtype='float32')\nfor i in range(len(words)):\n    if pre_em_model.__contains__(words[i]):\n        embedding_matrix[i]=pre_em_model.__getitem__(words[i])\n        \n#For pre-trained model require cells\n#end here","metadata":{"execution":{"iopub.status.busy":"2021-09-21T09:43:15.067329Z","iopub.execute_input":"2021-09-21T09:43:15.067709Z","iopub.status.idle":"2021-09-21T09:43:18.51429Z","shell.execute_reply.started":"2021-09-21T09:43:15.067677Z","shell.execute_reply":"2021-09-21T09:43:18.513094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Skip below two cell while use Pre-trained word embeddings**","metadata":{}},{"cell_type":"code","source":"#Custom train word embeddings start here\n#Fasttext word embeddings with spliting sentences\nem_model = FastText(vector_size=300, window=3, min_count=1,workers=16)\nem_model.build_vocab(sen)\nem_model.train(sen,total_examples=len(sen),epochs=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T06:46:43.05726Z","iopub.execute_input":"2021-08-20T06:46:43.057613Z","iopub.status.idle":"2021-08-20T06:46:49.028565Z","shell.execute_reply.started":"2021-08-20T06:46:43.05758Z","shell.execute_reply":"2021-08-20T06:46:49.027642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating embedding_matrix for loaded the 300 dim vector(this come form trained model)\n#representation of corpus dict \nembedding_matrix = np.zeros((len(words),300),dtype='float32')\nfor i in range(len(words)):\n    if em_model.wv.__contains__(words[i]):\n        embedding_matrix[i]=em_model.wv.__getitem__(words[i])\n\n#For custom train model require cells\n#end here","metadata":{"execution":{"iopub.status.busy":"2021-08-20T06:46:51.034004Z","iopub.execute_input":"2021-08-20T06:46:51.034324Z","iopub.status.idle":"2021-08-20T06:46:51.06569Z","shell.execute_reply.started":"2021-08-20T06:46:51.034294Z","shell.execute_reply":"2021-08-20T06:46:51.064818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape,Y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T05:52:50.888031Z","iopub.execute_input":"2021-09-20T05:52:50.888394Z","iopub.status.idle":"2021-09-20T05:52:50.895434Z","shell.execute_reply.started":"2021-09-20T05:52:50.888365Z","shell.execute_reply":"2021-09-20T05:52:50.894064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This test set for final model evaluation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T09:43:26.831733Z","iopub.execute_input":"2021-09-21T09:43:26.832179Z","iopub.status.idle":"2021-09-21T09:43:26.850333Z","shell.execute_reply.started":"2021-09-21T09:43:26.832149Z","shell.execute_reply":"2021-09-21T09:43:26.849287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train=np.array(y_train)\nprint(type(X_train),type(y_train))","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:09:41.028133Z","iopub.execute_input":"2021-09-20T12:09:41.028506Z","iopub.status.idle":"2021-09-20T12:09:41.054963Z","shell.execute_reply.started":"2021-09-20T12:09:41.02847Z","shell.execute_reply":"2021-09-20T12:09:41.054078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define per-fold score containers\nloss_per_fold = []\n\nnum_folds=5\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=num_folds, shuffle=True)\n\n\n# K-fold Cross Validation model evaluation\nfold_no = 1\nfor train, test in kfold.split(X_train, y_train):\n    epochs=20\n    units=70\n    batch_size=80\n\n    model = Sequential()\n    #in embeddings weights pushed the embeddings matrix that build\n    #from corpus dict and it's non trainable\n\n    #while use 'UNK' approach run below layer instead of another embedding layer\n    # model.add(Embedding(len(word_to_int),300,weights=[embedding_matrix],input_length=max_len,trainable=False))\n\n    model.add(Embedding(len(words),300,weights=[embedding_matrix],input_length=max_len,trainable=False))\n    model.add(Dropout(0.2))\n    model.add(Bidirectional(LSTM(units=units, return_sequences=True,recurrent_dropout=0.1)))\n    #softmax output layer\n    model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    history = model.fit(X_train[train], y_train[train], batch_size=batch_size, epochs=epochs,\n                        validation_data=(X_train[test],y_train[test]),\n                        verbose=0)\n    \n    print(\"For fold\"+str(fold_no)+\"loss Graph\")\n    from matplotlib import pyplot\n    pyplot.plot(history.history['loss'])\n    pyplot.plot(history.history['val_loss'])\n    pyplot.title('model train vs validation loss')\n    pyplot.ylabel('loss')\n    pyplot.xlabel('epoch')\n    pyplot.legend(['train', 'validation'], loc='upper right')\n    pyplot.show()\n    loss_per_fold.append(history.history['val_loss'])\n    # Increase fold no\n    fold_no += 1\n    \n    #This function contain evaluation of model\n    p = model.predict(X_train[test])\n    y_pred = np.argmax(p, axis=-1)\n    y_real=np.argmax(y_train[test],axis=-1)\n    print(\"Average: \"+str(f1_score(y_real.flatten(),y_pred.flatten(),average='weighted')))\n    print(\"Macro: \"+str(f1_score(y_real.flatten(),y_pred.flatten(),average='macro')))\n    print(\"Micro: \"+str(f1_score(y_real.flatten(),y_pred.flatten(),average='micro')))\n\n    print(classification_report(y_pred=y_pred.flatten(), y_true=y_real.flatten(),zero_division=0))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:09:41.056258Z","iopub.execute_input":"2021-09-20T12:09:41.0566Z","iopub.status.idle":"2021-09-20T12:22:08.37053Z","shell.execute_reply.started":"2021-09-20T12:09:41.056563Z","shell.execute_reply":"2021-09-20T12:22:08.369626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train\nepochs=20\nunits=75\nbatch_size=70\n\n#spiliting train/test data\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nmodel = Sequential()\n#in embeddings weights pushed the embeddings matrix that build\n#from corpus dict and it's non trainable\nmodel.add(Embedding(len(words),300,weights=[embedding_matrix],input_length=max_len,trainable=False))\nmodel.add(Dropout(0.2))\nmodel.add(Bidirectional(LSTM(units=units, return_sequences=True,recurrent_dropout=0.1)))\n#softmax output layer\nmodel.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nhistory = model.fit(X_train, np.array(y_train), batch_size=batch_size, epochs=epochs, validation_data=(X_test,np.array(y_test)), verbose=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-21T10:39:52.09661Z","iopub.execute_input":"2021-09-21T10:39:52.097073Z","iopub.status.idle":"2021-09-21T10:54:26.42831Z","shell.execute_reply.started":"2021-09-21T10:39:52.097009Z","shell.execute_reply":"2021-09-21T10:54:26.427192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model train vs validation loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T09:57:12.564679Z","iopub.execute_input":"2021-09-21T09:57:12.565116Z","iopub.status.idle":"2021-09-21T09:57:12.802304Z","shell.execute_reply.started":"2021-09-21T09:57:12.565084Z","shell.execute_reply":"2021-09-21T09:57:12.800907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This cell contain data information function\ndef counting_label(string):\n    a=0\n    for i in range(len(df['label'])):\n        if string==df['label'][i]:\n            a+=1\n    return a\n\ndef count(string):\n    b,i,o=\"B-\",\"I-\",\"O-\"\n    num_of_b=counting_label(b+string)\n    num_of_i=counting_label(i+string)\n    num_of_o=counting_label(o+string)\n    n_of_single_words=num_of_b-num_of_o        \n    n_of_double_words=num_of_b-n_of_single_words-num_of_i\n    n_of_multiple_words=num_of_i\n    return n_of_single_words,n_of_double_words,n_of_multiple_words\n\ndef data_details():\n    n_of_sen=counting_label(\"Break\")\n    n_of_words=len(df[\"word\"])-n_of_sen\n    n_of_misc_words=counting_label(\"0\")\n    n_of_single_words_name,n_of_double_words_name,n_of_multiple_words_name=count(\"PER\")\n    n_of_single_words_loc,n_of_double_words_loc,n_of_multiple_words_loc=count(\"LOC\")\n    n_of_single_words_org,n_of_double_words_org,n_of_multiple_words_org=count(\"ORG\")\n    n_of_single_words_time,n_of_double_words_time,n_of_multiple_words_time=count(\"TIME\")\n    n_of_single_words_unit,n_of_double_words_unit,n_of_multiple_words_unit=count(\"UNIT\")\n\n    d_dict={\"Person\":[n_of_single_words_name,n_of_double_words_name,n_of_multiple_words_name],\n          \"Location\":[n_of_single_words_loc,n_of_double_words_loc,n_of_multiple_words_loc],\n          \"Organization\":[n_of_single_words_org,n_of_double_words_org,n_of_multiple_words_org],\n          \"Time\":[n_of_single_words_time,n_of_double_words_time,n_of_multiple_words_time],\n          \"Unit\":[n_of_single_words_unit,n_of_double_words_unit,n_of_multiple_words_unit]\n         }\n    datas=pd.DataFrame(d_dict ,columns = ['Person','Location','Organization','Time','Unit'], index=['Single words','Double words','Multiple words'])\n\n    return n_of_sen,n_of_words,n_of_misc_words,datas\n\n#This function contain the labels distribution\ndef tag_details():\n    tag_distribution = data.groupby(\"label\").size().reset_index(name='counts')\n    print(tag_distribution)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T10:15:18.001588Z","iopub.execute_input":"2021-09-18T10:15:18.001966Z","iopub.status.idle":"2021-09-18T10:15:18.014005Z","shell.execute_reply.started":"2021-09-18T10:15:18.001936Z","shell.execute_reply":"2021-09-18T10:15:18.013051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"int_to_label = {value : key for (key, value) in label_to_int.items()}","metadata":{"execution":{"iopub.status.busy":"2021-09-21T08:01:03.318689Z","iopub.execute_input":"2021-09-21T08:01:03.319049Z","iopub.status.idle":"2021-09-21T08:01:03.3258Z","shell.execute_reply.started":"2021-09-21T08:01:03.319021Z","shell.execute_reply":"2021-09-21T08:01:03.324699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This function contain evaluation of model\np = model.predict(X_test)\ny_pred = np.argmax(p, axis=-1)\ny_real=np.argmax(y_test,axis=-1)\ny_pred=[int_to_label[i] for i in y_pred.flatten()]\ny_real=[int_to_label[i] for i in y_real.flatten()]\nprint(\"Average: \"+str(f1_score(y_real,y_pred,average='weighted')))\nprint(\"Macro: \"+str(f1_score(y_real,y_pred,average='macro')))\nprint(\"Micro: \"+str(f1_score(y_real,y_pred,average='micro')))\n\nc_re=classification_report(y_pred=y_pred, y_true=y_real,zero_division=0)\nprint(c_re)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T08:07:21.331917Z","iopub.execute_input":"2021-09-21T08:07:21.332228Z","iopub.status.idle":"2021-09-21T08:07:23.931987Z","shell.execute_reply.started":"2021-09-21T08:07:21.332199Z","shell.execute_reply":"2021-09-21T08:07:23.931065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nfrom pylab import rcParams","metadata":{"execution":{"iopub.status.busy":"2021-09-21T08:48:54.755658Z","iopub.execute_input":"2021-09-21T08:48:54.756018Z","iopub.status.idle":"2021-09-21T08:48:54.765685Z","shell.execute_reply.started":"2021-09-21T08:48:54.75597Z","shell.execute_reply":"2021-09-21T08:48:54.764695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm=confusion_matrix(y_real, y_pred, labels=tags)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T08:47:00.197572Z","iopub.execute_input":"2021-09-21T08:47:00.197894Z","iopub.status.idle":"2021-09-21T08:47:00.41766Z","shell.execute_reply.started":"2021-09-21T08:47:00.197864Z","shell.execute_reply":"2021-09-21T08:47:00.416797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='whitegrid', palette='bright', font_scale=1)\nrcParams['figure.figsize'] = 14, 8\nRANDOM_SEED = 42\nplt.figure(figsize=(12, 8))\nsns_plot=sns.heatmap(cm, xticklabels=tags, yticklabels=tags, annot=True, fmt=\"d\");\nplt.title(\"Bangla NER Classification\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T08:53:09.07336Z","iopub.execute_input":"2021-09-21T08:53:09.073752Z","iopub.status.idle":"2021-09-21T08:53:10.208581Z","shell.execute_reply.started":"2021-09-21T08:53:09.073721Z","shell.execute_reply":"2021-09-21T08:53:10.20775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name=\"72\"+\".hdf5\"\nsave_model(model, model_name)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T06:58:08.715616Z","iopub.execute_input":"2021-09-21T06:58:08.716109Z","iopub.status.idle":"2021-09-21T06:58:08.815395Z","shell.execute_reply.started":"2021-09-21T06:58:08.71607Z","shell.execute_reply":"2021-09-21T06:58:08.81445Z"},"trusted":true},"execution_count":null,"outputs":[]}]}