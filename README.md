# bangla-ner-bilstm-with-fasttext-word2vec-embeddings
This is a research work introduces a new dataset with 16 classes and six entities for Bangla NER that are developed from the archive of a leading Bengali newspaper available online. The data is embedded with two pre-trained word embeddings namely Word2vec and FastText and that are trained on accordingly 84.52 million and 1.3 billion words. For sequence to sequence, problems used the BiLSTM architecture. From our experiment in this approach, we evaluate two models with 5 fold cross-validation to prevent overfitting issues and after final evaluation we found the best F1 Macro score to be 0.72, F1 Micro score of 0.98.

